{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25eeabd",
   "metadata": {},
   "source": [
    "# The Bhakra Nangal Dam & Gobind Sagar Reservoir\n",
    "\n",
    "The [Bhakra Nangal dam](https://en.wikipedia.org/wiki/Bhakra_Dam) was opened in 1963 in India. The dam forms the Gobind Sagar reservoir and provides irrigation to 10 million acres in the neighboring states of Punjab, Haryana, and Rajasthan. We can use OPERA DSWx data to observe fluctuations in water levels over long time periods.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Bhakra_Dam_Aug_15_2008.JPG/440px-Bhakra_Dam_Aug_15_2008.JPG\"></img>\n",
    "</center>\n",
    "\n",
    "## Outline of steps for analysis\n",
    "\n",
    "+ Identifying search parameters\n",
    "  + AOI, time-window\n",
    "  + Endpoint, Provider, catalog identifier (\"short name\")\n",
    "+ Obtaining search results\n",
    "  + Instrospect, examine to identify features, bands of interest\n",
    "  + Wrap results into a DataFrame for easier exploration\n",
    "+ Exploring & refining search results\n",
    "  + Identify granules of highest value\n",
    "  + Filter extraneous granules with minimal contribution\n",
    "  + Assemble relevant filtered granules into DataFrame\n",
    "  + Identify kind of output to generate\n",
    "+ Data-wrangling to produce relevant output\n",
    "  + Download relevant granules into Xarray DataArray, stacked appropriately\n",
    "  + Do intermediate computations as necessary\n",
    "  + Assemble relevant data slices into visualization\n",
    "\n",
    "***\n",
    "\n",
    "### Preliminary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da468b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import rioxarray as rio\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for plotting\n",
    "import hvplot.pandas, hvplot.xarray\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b728f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAC imports to retrieve cloud data\n",
    "from pystac_client import Client\n",
    "from osgeo import gdal\n",
    "# GDAL setup for accessing cloud data\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF, TIFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae3417",
   "metadata": {},
   "source": [
    "### Convenient utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c023113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple utility to make a rectangle with given center of width dx & height dy\n",
    "def make_bbox(pt,dx,dy):\n",
    "    '''Returns bounding-box represented as tuple (x_lo, y_lo, x_hi, y_hi)\n",
    "    given inputs pt=(x, y), width & height dx & dy respectively,\n",
    "    where x_lo = x-dx/2, x_hi=x+dx/2, y_lo = y-dy/2, y_hi = y+dy/2.\n",
    "    '''\n",
    "    return tuple(coord+sgn*delta for sgn in (-1,+1) for coord,delta in zip(pt, (dx/2,dy/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801831ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple utility to plot an AOI or bounding-box\n",
    "def plot_bbox(bbox):\n",
    "    '''Given bounding-box, returns GeoViews plot of Rectangle & Point at center\n",
    "    + bbox: bounding-box specified as (lon_min, lat_min, lon_max, lat_max)\n",
    "    Assume longitude-latitude coordinates.\n",
    "    '''\n",
    "    # These plot options are fixed but can be over-ridden\n",
    "    point_opts = opts.Points(size=12, alpha=0.25, color='blue')\n",
    "    rect_opts = opts.Rectangles(line_width=0, alpha=0.1, color='red')\n",
    "    lon_lat = (0.5*sum(bbox[::2]), 0.5*sum(bbox[1::2]))\n",
    "    return (gv.Points([lon_lat]) * gv.Rectangles([bbox])).opts(point_opts, rect_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5341a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility to extract search results into a Pandas DataFrame\n",
    "def search_to_dataframe(search):\n",
    "    '''Constructs Pandas DataFrame from PySTAC Earthdata search results.\n",
    "    DataFrame columns are determined from search item properties and assets.\n",
    "    'asset': string identifying an Asset type associated with a granule\n",
    "    'href': data URL for file associated with the Asset in a given row.'''\n",
    "    granules = list(search.items())\n",
    "    assert granules, \"Error: empty list of search results\"\n",
    "    props = list({prop for g in granules for prop in g.properties.keys()})\n",
    "    tile_ids = map(lambda granule: granule.id.split('_')[3], granules)\n",
    "    rows = (([g.properties.get(k, None) for k in props] + [a, g.assets[a].href, t])\n",
    "                for g, t in zip(granules,tile_ids) for a in g.assets )\n",
    "    df = pd.concat(map(lambda x: pd.DataFrame(x, index=props+['asset','href', 'tile_id']).T, rows),\n",
    "                   axis=0, ignore_index=True)\n",
    "    assert len(df), \"Empty DataFrame\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility to remap pixel values to a sequence of contiguous integers\n",
    "def relabel_pixels(data, values, null_val=255, transparent_val=0, replace_null=True, start=0):\n",
    "    \"\"\"\n",
    "    This function accepts a DataArray with a finite number of categorical values as entries.\n",
    "    It reassigns the pixel labels to a sequence of consecutive integers starting from start.\n",
    "    data:            Xarray DataArray with finitely many categories in its array of values.\n",
    "    null_val:        (default 255) Pixel value used to flag missing data and/or exceptions.\n",
    "    transparent_val: (default 0) Pixel value that will be fully transparent when rendered.\n",
    "    replace_null:    (default True) Maps null_value->transparent_value everywhere in data.\n",
    "    start:           (default 0) starting range of consecutive integer values for new labels.\n",
    "    The values returned are:\n",
    "    new_data:        Xarray DataArray containing pixels with new values\n",
    "    relabel:         dictionary associating old pixel values with new pixel values\n",
    "    \"\"\"\n",
    "    new_data = data.copy(deep=True)\n",
    "    if values:\n",
    "        values = np.sort(np.array(values, dtype=np.uint8))\n",
    "    else:\n",
    "        values = np.sort(np.unique(data.values.flatten()))\n",
    "    if replace_null:\n",
    "        new_data = new_data.where(new_data!=null_val, other=transparent_val)\n",
    "        values = values[np.where(values!=null_val)]\n",
    "    n_values = len(values)\n",
    "    new_values = np.arange(start=start, stop=start+n_values, dtype=values.dtype)\n",
    "    assert transparent_val in new_values, f\"{transparent_val=} not in {new_values}\"\n",
    "    relabel = dict(zip(values, new_values))\n",
    "    for old, new in relabel.items():\n",
    "        if new==old: continue\n",
    "        new_data = new_data.where(new_data!=old, other=new)\n",
    "    return new_data, relabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce4873",
   "metadata": {},
   "source": [
    "These functions could be placed in module files for more developed research projects. For learning purposes, they are embedded within this notebook.\n",
    "\n",
    "***\n",
    "\n",
    "## Identifying search parameters\n",
    "\n",
    "For coordinates of the dam, we'll use $(76.46^{\\circ}, 31.42^{\\circ})$. We'll also look for a full calendar year's worth of data between April 1, 2023 and April 1, 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AOI = make_bbox((76.46, 31.42), 0.2, 0.2)\n",
    "DATE_RANGE = \"2023-04-01/2024-04-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ec788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally plot the AOI\n",
    "basemap = gv.tile_sources.OSM(alpha=0.5, padding=0.1)\n",
    "plot_bbox(AOI) * basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeccc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = dict(bbox=AOI, datetime=DATE_RANGE)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27091a52",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Obtaining search results\n",
    "\n",
    "We're going to look for OPERA DSWx data products, so we define the `ENDPOINT`, `PROVIDER`, and `COLLECTIONS` as follows (these values are occasionally modified, so some searching through NASA's [Earthdata Search website](https://search.earthdata.nasa.gov) may be necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fabc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "PROVIDER = \"POCLOUD\"\n",
    "COLLECTIONS = [\"OPERA_L3_DSWX-HLS_V1_1.0\"]\n",
    "# Update the dictionary opts with list of collections to search\n",
    "search_params.update(collections=COLLECTIONS)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "catalog = Client.open(f'{ENDPOINT}/{PROVIDER}/')\n",
    "search_results = catalog.search(**search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f935d",
   "metadata": {},
   "source": [
    "Having executed the search, the results can be perused in a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = search_to_dataframe(search_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d10227",
   "metadata": {},
   "source": [
    "We'll clean the `DataFrame` `df` by renaming the `eo:cloud_cover` column, dropping the extra datetime columns, converting the datatypes sensibly, and setting the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'eo:cloud_cover':'cloud_cover'})\n",
    "df.cloud_cover = df.cloud_cover.astype(np.float16)\n",
    "df = df.drop(['start_datetime', 'end_datetime'], axis=1)\n",
    "df = df.convert_dtypes()\n",
    "df.datetime = pd.DatetimeIndex(df.datetime)\n",
    "df = df.set_index('datetime').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9cc9a1",
   "metadata": {},
   "source": [
    "At this stage, the `DataFrame` of search results has over two thousand rows. Let's trim that down.\n",
    "\n",
    "***\n",
    "\n",
    "## Exploring & refining search results\n",
    "\n",
    "We'll filter the rows of `df` to capture only granules captured with less than 10% cloud cover and the `B01_WTR` band of the DSWx data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b5f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = df.cloud_cover<10\n",
    "c2 = df.asset.str.contains('B01_WTR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[c1 & c2]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9e576",
   "metadata": {},
   "source": [
    "We can count all the distinct entries of the `tile_id` column and. find that there's only one (`T43RFQ`). This means that the AOI specified lies strictly inside a single MGRS tile and that all granules found will be associated with that particular geographic tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tile_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e081437",
   "metadata": {},
   "source": [
    "We've reduced the total number of granules to a little over fifty. Let's use these to produce a visualization.\n",
    "\n",
    "***\n",
    "\n",
    "## Data-wrangling to produce relevant output\n",
    "\n",
    "As we've seen several times now, we'll stack the two-dimensional arrays from the GeoTIFF files listed in `df.href` into a three-dimensional `DataArray`; we'll use the identifier `stack` to label the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07541b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stack = []\n",
    "for timestamp, row in df.iterrows():\n",
    "    data = rio.open_rasterio(row.href).squeeze()\n",
    "    data = data.rename(dict(x='longitude', y='latitude'))\n",
    "    del data.coords['band']\n",
    "    data.coords.update({'time':timestamp})\n",
    "    data.attrs = dict(description=f\"OPERA DIST: VEG-DIST-STATUS\", units=None)\n",
    "    stack.append(data)\n",
    "stack = xr.concat(stack, dim='time')\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf352cb",
   "metadata": {},
   "source": [
    "We can see the pixel values that actually occur in the array `stack` using the NumPy `unique` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44382a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4373d",
   "metadata": {},
   "source": [
    "As a reminder, according to the [DSWx product specification](https://d2pn8kiwq2w21t.cloudfront.net/documents/ProductSpec_DSWX_URS309746.pdf), the meanings of the raster values are as follows:\n",
    "\n",
    "+ **0**: Not Water&mdash;any area with valid reflectance data that is not from one of the other allowable categories (open water, partial surface water, snow/ice, cloud/cloud shadow, or ocean masked).\n",
    "+ **1**: Open Water&mdash;any pixel that is entirely water unobstructed to the sensor, including obstructions by vegetation, terrain, and buildings.\n",
    "+ **2**: Partial Surface Water&mdash;an area that is at least 50% and less than 100% open water (e.g., inundated sinkholes, floating vegetation, and pixels bisected by coastlines).\n",
    "+ **252**: Snow/Ice.\n",
    "+ **253**: Cloud or Cloud Shadow&mdash;an area obscured by or adjacent to cloud or cloud shadow.\n",
    "+ **254**: Ocean Masked&mdash;an area identified as ocean using a shoreline database with an added margin.\n",
    "+ **255**: Fill value (missing data).\n",
    "\n",
    "Notice that the value `254`&mdash;ocean masked&mdash;does not occur in this particular collection of rasters because this particular region is far away from the coast.\n",
    "\n",
    "To clean up the data (in case we want to use a colormap), let's reassign the pixel values with our utility function `relabel_pixels`. This time, let's keep the \"no data\" (`255`) values so we can see where data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack, relabel = relabel_pixels(stack, values=[0,1,2,252,253,255], replace_null=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58792444",
   "metadata": {},
   "source": [
    "We can execute `np.unique` again to ensure that the data has been modified as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c33efcc",
   "metadata": {},
   "source": [
    "Let's now assign a colormap to help visualize the raster images. In this instance, the colormap uses several distinct colors with full opacity and black, partially transparent pixels to indicate missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a colormap using RGBA values; these need to be written manually here...\n",
    "COLORS = {\n",
    "0: (255, 255, 255, 0.0),  # Not Water\n",
    "1: (  0,   0, 255, 1.0),  # Open Water\n",
    "2: (180, 213, 244, 1.0),  # Partial Surface Water\n",
    "3: (  0, 255, 255, 1.0),  # Snow/Ice\n",
    "4: (175, 175, 175, 1.0),  # Cloud/Cloud Shadow\n",
    "5: (  0,   0, 0, 0.5),    # Missing\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97c8e4",
   "metadata": {},
   "source": [
    "We're ready to plot the data.\n",
    "\n",
    "+ We define suitable options in the dictionaries `image_opts` and `layout_opts`.\n",
    "+ We construct an object `view` that consists of slices extracted from `stack` by subsampling every `steps` pixels (reduce `steps` to `1` or `None` to view the rasters at full resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95532cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_opts = dict(  \n",
    "                    x='longitude',\n",
    "                    y='latitude',\n",
    "                    cmap = list(COLORS.values()),\n",
    "                    colorbar=False,\n",
    "                    tiles = gv.tile_sources.OSM,\n",
    "                    tiles_opts=dict(padding=0.05, alpha=0.25),\n",
    "                    project=True,\n",
    "                    rasterize=True, \n",
    "                    framewise=False,\n",
    "                    widget_location='bottom',\n",
    "                 )\n",
    "\n",
    "layout_opts = dict(\n",
    "                    title = 'Bhakra Nangal Dam, India - water extent over a year',\n",
    "                    xlabel='Longitude (degrees)',\n",
    "                    ylabel='Latitude (degrees)',\n",
    "                    fontscale=1.25,\n",
    "                    frame_width=500, \n",
    "                    frame_height=500\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c555dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 100\n",
    "subset = slice(0,None,steps)\n",
    "view = stack.isel(longitude=subset, latitude=subset)\n",
    "view.hvplot.image( **image_opts, **layout_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8def3a4",
   "metadata": {},
   "source": [
    "The visualization above may take a wile to update (depending on the choice of `steps`). It does provide a way to see the water accumulation over a period of a year. There are a number of slices in which a lot of data is missing, so some care is required to interpret those time slices.\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
